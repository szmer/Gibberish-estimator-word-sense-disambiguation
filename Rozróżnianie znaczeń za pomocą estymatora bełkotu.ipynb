{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_file = \"/home/szymon/lingwy/nkjp/nkjp_index.txt\"\n",
    "nkjp_path = \"/home/szymon/lingwy/nkjp/pełny/\"\n",
    "vecs_path = \"/home/szymon/lingwy/nkjp/wektory/nkjp+wiki-lemmas-all-100-skipg-ns.txt/\"\n",
    "vecs_dim = 100\n",
    "window_size = 6 # how many words to condider on both sides of the target\n",
    "batch_size = window_size * 2\n",
    "corp_runs = 8\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fragms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NKJP fragments\n",
    "from lxml import etree\n",
    "import os.path\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "with open(index_file) as index:\n",
    "    for fragm_id in index:\n",
    "        filepath = nkjp_path+fragm_id.strip()+'/ann_words.xml'\n",
    "        if not os.path.isfile(filepath): # there are two versions of name of these files\n",
    "            filepath = nkjp_path+fragm_id.strip()+'/ann_named.xml'\n",
    "        if not os.path.isfile(filepath):\n",
    "            print('Note: cannot access {}'.format(fragm_id.strip()))\n",
    "            continue\n",
    "        fragms += [[]]\n",
    "        \n",
    "        tree = etree.parse(filepath)\n",
    "         # tag is namespaced, .// for finding anywhere in the tree\n",
    "        for elem in tree.iterfind('.//{http://www.tei-c.org/ns/1.0}f[@name]'):\n",
    "            if elem.attrib['name'] == 'base':\n",
    "                fragms[-1].append(elem[0].text) # first child <string>\n",
    "                unique_words.add(elem[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zatrzasnąć', 'drzwi', 'od', 'mieszkanie', ',', 'dwa', 'raz', 'przekręcić', 'klucz', ',', 'nacisnąć', 'klamka', ',', 'by', 'sprawdzić', ',', 'czy', 'dobrze', 'zamknąć', ',', 'zbiec', 'po', 'schody', ',', 'minąć', 'furtka', ',', 'także', 'on', 'zamknąć', ',', 'i', 'znaleźć się', 'na', 'wąski', 'uliczka', 'między', 'ogródek', ',', 'gdzie', 'drzemać', 'w', 'majowy', 'słońce', 'trójkątny', 'ciemnozielony', 'świerk', ',', 'jaki', 'być', 'w pobliżu', 'on', 'dom', '.', 'bohater', 'powieść', 'Paźniewski', 'być', 'miasto', ',', 'Krzemieniec', '.', 'jak', 'za', 'czas', 'Słowacki', 'funkcjonować', 'liceum', 'i', 'płynąć', 'Ikwa', '.', 'Krzemieniec', 'powieściowy', 'być', 'tamten', 'Krzemieniec', ',', 'ale', 'być', 'także', 'miasto', 'wywołać', 'z', 'osobisty', 'pamięć', 'Paźniewski', '.', 'swój', 'droga', 'do', 'ten', 'miasto', 'autor', '„', 'krótki', 'dzień', '”', 'zacząć', 'z daleka', 'bardzo', '.', '„', 'nigdy', 'być', 'w', 'ten', 'dom', ',', 'a', 'przecież', 'wszystko', 'pamiętać', 'doskonale', '”', '.', 'ale', 'dzisiaj', '?', 'jaki', 'dzisiaj', 'odegrać', 'rola', 'poetyka', 'Przyboś', '?', 'oczywiście', ',', 'już', 'sam', 'fakt', 'on', 'istnieć', 'być', 'wartość', '.', 'nasz', 'literatura', ',', 'bogaty', 'w', 'improwizacja', 'i', 'w', 'akt', 'strzelisty', ',', 'być', 'ubogi', 'w', 'teoria', '.', 'rola', 'teoretyk', 'spełniać', 'felietonista', ',', 'który', 'co', 'tydzień', 'fundować', 'szkoła', 'i', 'formułować', 'program', '.', 'dlatego', 'założenie', 'teoretyczny', 'Przyboś', 'obok', 'teoria', 'Peiper', 'i', 'Witkacy', ',', 'a', 'równolegle', 'do', 'propozycja', 'system', 'Irzykowski', 'i', 'Sandauer', ',', 'stanowić', 'kapitał', 'nasz', 'myśl', 'krytyczny', ',', 'naturalny', 'fundament', 'każdy', 'twórczość', '.', 'to', 'oczywistość', '.', 'Halina', 'Auderska', 'w', 'wszystek', 'książka', 'kazać', 'swój', 'bohater', 'szukać', 'tożsamość', '.', 'w', '„', 'babi', 'lato', '”', 'mieć', 'odwaga', 'uznać', 'za', 'istotny', 'kryterium', 'tożsamość', 'poczucie', 'przynależność', 'nie, nie, lecz', 'do', 'idea', 'do', 'kultura', 'i', 'mit', 'narodowy', 'do', 'to', ',', 'co', 'być', 'podstawa', 'byt', 'każdy', 'człowieczeństwo', '.', 'miejsce', 'na', 'ziemia', ',', 'konkret', 'fizyczny', 'i', 'społeczny', 'jednocześnie', 'być', 'to', ',', 'co', 'stanowić', 'o', 'wymiar', 'życie', 'i', 'los', '.', 'Paźniewski', 'w', '„', 'krótki', 'dzień', '”', 'ofiarować', 'Kresy', 'nie', 'mało', ',', 'niż', 'z', 'on', 'zaczerpnąć', '.', 'zatrzymać', 'potop', '.', 'zamówić', 'kataklizm', '.', 'stworzyć', 'wizja', 'oczekiwanie', ',', 'wizja', 'spokój', 'przed', 'burza', '.', 'z', 'napięcie', 'czekać', 'na', 'chwila', ',', 'który', 'być', 'chwila', 'decydować', 'o', 'los', 'bohater', '.', 'cóż za', 'ulga', '.', 'mijać', 'ostatni', 'zdanie', 'powieść', '.', 'co za', 'wspaniały', 'książka', '!', 'ocalać', '!', '„plama”', 'Piętak', ',', 'jeden', 'spośród', 'kilka', 'znakomity', 'współczesny', 'powieść', ',', 'także', 'ze względu na', 'on', 'zaklasyfikować', 'wraz z', 'cały', 'twórczość', 'ten', 'pisarz', 'do', 'nurt', 'wiejski', ',', 'mieć', 'w', 'odbiór', 'powszechny', 'ten', 'ranga', ',', 'jaki', 'rzeczywiście', 'posiadać', '.', 'wszystko', 'co', 'Piętak', 'wynieść', 'z', 'chłopski', 'szkoła', 'wyobraźnia', ',', 'zaowocować', 'w', '„plama”', 'wysoki', 'subtelność', 'psychologiczny', ',', 'na', 'jaki', 'stać', 'literatura', '.', 'to', 'już', 'wątpliwość', 'religijny', ',,', 'ten', 'wątpliwość', 'pierwszy', 'stopień', 'wtajemniczenie', 'w', 'sprawa', 'świat', 'wątpliwość', '„', 'niebo', 'w', 'płomień', '”', 'czy', '„', 'Jan', 'Barois', '”', '.', 'tu', 'chodzić', 'o', 'sensowność', 'dogmat', 'czy', 'ścisłość', 'religijny', 'wyobrażenie', ',', 'chodzić', 'już', 'o', 'religia', ',', 'o', 'tajemnica', 'stworzenie', ',', 'ale', 'o', 'norma', 'etyczny', '.', 'kto', 'on', 'ustanowić', ',', 'kiedy', 'Bóg', 'zabraknąć', '?']\n"
     ]
    }
   ],
   "source": [
    "print(fragms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_line = True\n",
    "word_n = 0\n",
    "word_idx = {}\n",
    "\n",
    "# we'll read those from the data file\n",
    "vecs_count = 0\n",
    "vecs_dim = 100\n",
    "\n",
    "with open(vecs_path+\"data\") as vecs_file:\n",
    "    for line in vecs_file:\n",
    "        if first_line:\n",
    "            vecs_count = int(line.split(' ')[0])\n",
    "            vecs_dim = int(line.split(' ')[1])\n",
    "            first_line = False\n",
    "            continue\n",
    "        word_idx[line.split(' ')[0]] = word_n\n",
    "        word_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_idx = {w: num for (num, w) in enumerate(list(unique_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vecs_count = len(unique_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vecs = np.loadtxt(vecs_path+\"data\", encoding=\"utf-8\",\n",
    "                       dtype=np.float32, # tensorflow's requirement\n",
    "                       skiprows=1, usecols=tuple(range(1, vecs_dim+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dummy boundary/unknown marker.\n",
    "word_vecs = np.vstack([word_vecs, np.zeros((1,vecs_dim), dtype=np.float32)])\n",
    "vecs_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_id(word):\n",
    "    return word_idx[word] if word in word_idx else vecs_count-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1549322"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id('ffggf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1549323, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_count, vecs_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "fragms_step = 1 # set to higher values if we want to skip some proportion of fragments\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vecs_count - 1 # the zero additional vector\n",
    "\n",
    "def skipgram_batches():\n",
    "    for run_n in range(corp_runs):\n",
    "        sent_n = 0\n",
    "        word_n = 0\n",
    "        \n",
    "        target_n = 0 # relative to the current batch\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        \n",
    "        while sent_n < len(fragms):\n",
    "            for j in range(window_size):\n",
    "                batch[target_n*window_size+j] = word_id(fragms[sent_n][word_n])\n",
    "            # \"Good\" examples - words near the target (we will let TensorFlow randomize the \"bad\" ones)\n",
    "            for j in range(window_size // 2):\n",
    "                labels[target_n*window_size+j*2] = (word_id(fragms[sent_n][word_n-j-1]) if word_n-j-1 >= 0\n",
    "                                                       else bound_token_id)\n",
    "                labels[target_n*window_size+j*2+1] = (word_id(fragms[sent_n][word_n+j+1])\n",
    "                                                         if word_n+j+1 < len(fragms[sent_n])\n",
    "                                                         else bound_token_id)\n",
    "                \n",
    "            target_n += 1\n",
    "            if target_n == (batch_size // window_size):\n",
    "                yield batch, labels\n",
    "                batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                target_n = 0\n",
    "                \n",
    "            word_n += 1\n",
    "            try:\n",
    "                while word_n == len(fragms[sent_n]):\n",
    "                    word_n = 0\n",
    "                    sent_n += fragms_step\n",
    "                    if (floor(sent_n / len(fragms) * 10)\n",
    "                        > floor((sent_n-fragms_step) / len(fragms) * 10)):\n",
    "                        print('{}0%'.format(floor(sent_n / len(fragms) * 10)), end=' ')\n",
    "            except IndexError: # happens on the end of the corpus\n",
    "                break\n",
    "                \n",
    "        batch[target_n:] = 0.0\n",
    "        labels[target_n:, :] = 0.0\n",
    "        yield batch, labels#, (run_n == corp_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classif_W = Variable(torch.randn(vecs_dim*2, 1), requires_grad=True)\n",
    "classif_b = Variable(torch.randn(1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2018-02-15 13:24:24.740099\n",
      "Loss: Variable containing:\n",
      "-0.1500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2314\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2235\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2304\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2160\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2276\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2159\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2106\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2404\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      "-0.2239\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-357a5b862974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_n\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "batch_n = 0\n",
    "\n",
    "print('Training start:', datetime.datetime.now())\n",
    "for batch, labels in skipgram_batches():\n",
    "    # We get word indices, convert them to vectors.\n",
    "    batch = word_vecs[batch] # main words\n",
    "    pos_examples = word_vecs[labels]\n",
    "    neg_examples = word_vecs[np.random.randint(vecs_count, size=pos_examples.shape)]\n",
    "    \n",
    "    pos_batch = Variable(torch.Tensor(np.hstack((batch, pos_examples))), requires_grad=False)\n",
    "    pos_preds = (pos_batch.mm(classif_W) + classif_b).sigmoid()\n",
    "    neg_batch = Variable(torch.Tensor(np.hstack((batch, pos_examples))), requires_grad=False)\n",
    "    neg_preds = (neg_batch.mm(classif_W) + classif_b).sigmoid()\n",
    "    \n",
    "    loss1 = (- pos_preds).sum() / batch_size\n",
    "    loss1.backward()\n",
    "    loss2 = neg_preds.sum() / (batch_size * 2)\n",
    "    loss2.backward()\n",
    "    if batch_n % 5000 == 0:\n",
    "        print(\"Loss: {}\".format(loss1 + loss2))\n",
    "        #print(pos_preds)\n",
    "        #print(neg_preds)\n",
    "    \n",
    "    classif_W.data = learning_rate * classif_W.grad.data\n",
    "    classif_b.data = learning_rate * classif_b.grad.data\n",
    "    classif_W.grad.data.zero_()\n",
    "    classif_b.grad.data.zero_()\n",
    "    \n",
    "    batch_n += 1\n",
    "print('Training end:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow (currently unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # Model parameters: word embeddings and model weights & biases for each word.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vecs_count, vecs_dim], -1.0, 1.0))\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vecs_count, vecs_dim],\n",
    "                                                  stddev=1.0 / math.sqrt(vecs_dim)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vecs_count]))\n",
    "    \n",
    "    # The computation graph.\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "    \n",
    "    # Number of random words to sample apart from the true target; the model should learn to\n",
    "    # assign low probability to them given the context.\n",
    "    negative_samples_n = batch_size\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=labels,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         num_sampled=negative_samples_n,\n",
    "                                         num_classes=vecs_count))\n",
    "    # Vanilla SGD seems to work here better - since we train practically a different word vector\n",
    "    # each time, decaying momentum hinders training of later vectors before they can even be shown\n",
    "    # to the net, especially in the case of Adagrad's vanishing updates.\n",
    "    # (NOTE!) here we DO NOT touch the embeddings, we want to only learn nce_weights and biases!\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2018-02-07 19:02:51.118178\n",
      "(loss: 675.9548950195312) 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% (loss: 2.3737263679504395) 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% (loss: 1.877175211906433) 60% 70% 80% 90% 100% Final loss: 2.2043426\n",
      "Training end: 2018-02-07 19:25:10.739346\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# we want to use those later:\n",
    "trained_nce_weights = []\n",
    "trained_nce_biases = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Training start:', datetime.datetime.now())\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    for batch_inputs, batch_labels, is_last in skipgram_batches():\n",
    "        if is_last:\n",
    "            _, loss_val, trained_nce_weights, trained_nce_biases = sess.run(\n",
    "                [optimizer, loss, nce_weights, nce_biases],\n",
    "                feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "            print('Training end:', datetime.datetime.now())\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            if (i % 250000 == 0):\n",
    "                print('(loss: {})'.format(loss_val), end=' ')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55766, 100), (55766,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_nce_weights.shape, trained_nce_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_words = [rev_word_idx[i] if i < vecs_count-1 else ' ' for i in range(idxs.shape[1])][::-1] # reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = \"być\"\n",
    "vec = np.reshape(word_vecs[word_id(word), :], (1, vecs_dim))\n",
    "prediction = np.dot(vec, np.transpose(trained_nce_weights))\n",
    "prediction = np.add(prediction, trained_nce_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31166"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs = np.argsort(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 55766)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'kazirodczy',\n",
       " 'teori',\n",
       " 'nscrossen',\n",
       " 'Bierówka',\n",
       " 'hat-tricka',\n",
       " 'bazgrać',\n",
       " 'rozgościć',\n",
       " 'niezmącony',\n",
       " 'Valle',\n",
       " 'łajać',\n",
       " '7-13',\n",
       " 'Brett',\n",
       " 'Silesii',\n",
       " 'zaliczkowy',\n",
       " 'Wojcieszów',\n",
       " 'Sław',\n",
       " 'odwiecznie',\n",
       " '²',\n",
       " 'kwalifikator',\n",
       " 'stulejka',\n",
       " 'Eugen',\n",
       " 'avi',\n",
       " 'abażur',\n",
       " 'wydeptywać',\n",
       " 'Dederko',\n",
       " 'płatowiec',\n",
       " 'niedosłuch',\n",
       " 'Kornasiewicz',\n",
       " 'Krysiewicza',\n",
       " 'Besbir',\n",
       " 'separować',\n",
       " 'Rumsfeld',\n",
       " 'Kurowo',\n",
       " 'obstrzał',\n",
       " 'Galapagos',\n",
       " 'odmownie',\n",
       " 'Dłutów',\n",
       " 'wolnomularski',\n",
       " 'Liptak',\n",
       " 'zbutwiały',\n",
       " 'Gliwa',\n",
       " 'Johana',\n",
       " 'tia',\n",
       " 'Cieślikowski',\n",
       " 'doktrynerstwo',\n",
       " 'nieokrzesany',\n",
       " '1440',\n",
       " 'EG',\n",
       " 'Lokia',\n",
       " 'Elo',\n",
       " 'budzetu',\n",
       " 'UFK',\n",
       " '70-300',\n",
       " 'wszystkowiedzący',\n",
       " 'Burger',\n",
       " 'ankiete',\n",
       " 'Małastowskiej',\n",
       " 'Kubiś',\n",
       " 'rozpustny',\n",
       " 'Galos',\n",
       " '75-300',\n",
       " 'JKK',\n",
       " '637-12-23',\n",
       " 'płaszczak',\n",
       " 'wine',\n",
       " 'ats',\n",
       " 'żupan',\n",
       " 'zauwazyl',\n",
       " 'wykusz',\n",
       " \"There's\",\n",
       " 'Solar',\n",
       " 'skamleć',\n",
       " 'szczotkować',\n",
       " 'Karamazow',\n",
       " 'gromnica',\n",
       " 'dysgrafia',\n",
       " 'łyżworolka',\n",
       " 'Zarzycka',\n",
       " 'Muskat',\n",
       " 'choleryk',\n",
       " 'Matteo',\n",
       " 'Basiak',\n",
       " 'browning',\n",
       " 'wywiadownia',\n",
       " 'Ravensbrück',\n",
       " 'Lubomirska',\n",
       " 'KDE',\n",
       " 'Claus',\n",
       " 'Hucina',\n",
       " 'Wizygot',\n",
       " 'Hg',\n",
       " 'Knysok',\n",
       " 'zaparzać',\n",
       " 'odbezpieczyć',\n",
       " 'Sanders',\n",
       " 'Kriegsmarine',\n",
       " 'Record',\n",
       " 'zapamiętale',\n",
       " 'Paper']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
