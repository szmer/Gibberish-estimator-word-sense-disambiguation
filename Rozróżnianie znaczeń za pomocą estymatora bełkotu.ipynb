{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_file = \"/home/szymon/lingwy/nkjp/nkjp_index.txt\"\n",
    "nkjp_path = \"/home/szymon/lingwy/nkjp/pełny/\"\n",
    "vecs_path = \"/home/szymon/lingwy/nkjp/wektory/nkjp+wiki-lemmas-all-100-skipg-ns.txt/\"\n",
    "vecs_dim = 100\n",
    "window_size = 4 # how many words to condider on both sides of the target\n",
    "batch_size = window_size * 2\n",
    "corp_runs = 2\n",
    "learning_rate = 0.3\n",
    "reg_rate = 0.005\n",
    "points_per_neg_sample = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fragms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the NKJP fragments\n",
    "from lxml import etree\n",
    "import os.path\n",
    "\n",
    "unique_words = set()\n",
    "words_count = 0\n",
    "\n",
    "with open(index_file) as index:\n",
    "    for fragm_id in index:\n",
    "        filepath = nkjp_path+fragm_id.strip()+'/ann_words.xml'\n",
    "        if not os.path.isfile(filepath): # there are two versions of name of these files\n",
    "            filepath = nkjp_path+fragm_id.strip()+'/ann_named.xml'\n",
    "        if not os.path.isfile(filepath):\n",
    "            print('Note: cannot access {}'.format(fragm_id.strip()))\n",
    "            continue\n",
    "        fragms += [[]]\n",
    "        \n",
    "        tree = etree.parse(filepath)\n",
    "         # tag is namespaced, .// for finding anywhere in the tree\n",
    "        for elem in tree.iterfind('.//{http://www.tei-c.org/ns/1.0}f[@name]'):\n",
    "            if elem.attrib['name'] == 'base':\n",
    "                fragms[-1].append(elem[0].text) # first child <string>\n",
    "                words_count += 1\n",
    "                unique_words.add(elem[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zatrzasnąć', 'drzwi', 'od', 'mieszkanie', ',', 'dwa', 'raz', 'przekręcić', 'klucz', ',', 'nacisnąć', 'klamka', ',', 'by', 'sprawdzić', ',', 'czy', 'dobrze', 'zamknąć', ',', 'zbiec', 'po', 'schody', ',', 'minąć', 'furtka', ',', 'także', 'on', 'zamknąć', ',', 'i', 'znaleźć się', 'na', 'wąski', 'uliczka', 'między', 'ogródek', ',', 'gdzie', 'drzemać', 'w', 'majowy', 'słońce', 'trójkątny', 'ciemnozielony', 'świerk', ',', 'jaki', 'być', 'w pobliżu', 'on', 'dom', '.', 'bohater', 'powieść', 'Paźniewski', 'być', 'miasto', ',', 'Krzemieniec', '.', 'jak', 'za', 'czas', 'Słowacki', 'funkcjonować', 'liceum', 'i', 'płynąć', 'Ikwa', '.', 'Krzemieniec', 'powieściowy', 'być', 'tamten', 'Krzemieniec', ',', 'ale', 'być', 'także', 'miasto', 'wywołać', 'z', 'osobisty', 'pamięć', 'Paźniewski', '.', 'swój', 'droga', 'do', 'ten', 'miasto', 'autor', '„', 'krótki', 'dzień', '”', 'zacząć', 'z daleka', 'bardzo', '.', '„', 'nigdy', 'być', 'w', 'ten', 'dom', ',', 'a', 'przecież', 'wszystko', 'pamiętać', 'doskonale', '”', '.', 'ale', 'dzisiaj', '?', 'jaki', 'dzisiaj', 'odegrać', 'rola', 'poetyka', 'Przyboś', '?', 'oczywiście', ',', 'już', 'sam', 'fakt', 'on', 'istnieć', 'być', 'wartość', '.', 'nasz', 'literatura', ',', 'bogaty', 'w', 'improwizacja', 'i', 'w', 'akt', 'strzelisty', ',', 'być', 'ubogi', 'w', 'teoria', '.', 'rola', 'teoretyk', 'spełniać', 'felietonista', ',', 'który', 'co', 'tydzień', 'fundować', 'szkoła', 'i', 'formułować', 'program', '.', 'dlatego', 'założenie', 'teoretyczny', 'Przyboś', 'obok', 'teoria', 'Peiper', 'i', 'Witkacy', ',', 'a', 'równolegle', 'do', 'propozycja', 'system', 'Irzykowski', 'i', 'Sandauer', ',', 'stanowić', 'kapitał', 'nasz', 'myśl', 'krytyczny', ',', 'naturalny', 'fundament', 'każdy', 'twórczość', '.', 'to', 'oczywistość', '.', 'Halina', 'Auderska', 'w', 'wszystek', 'książka', 'kazać', 'swój', 'bohater', 'szukać', 'tożsamość', '.', 'w', '„', 'babi', 'lato', '”', 'mieć', 'odwaga', 'uznać', 'za', 'istotny', 'kryterium', 'tożsamość', 'poczucie', 'przynależność', 'nie, nie, lecz', 'do', 'idea', 'do', 'kultura', 'i', 'mit', 'narodowy', 'do', 'to', ',', 'co', 'być', 'podstawa', 'byt', 'każdy', 'człowieczeństwo', '.', 'miejsce', 'na', 'ziemia', ',', 'konkret', 'fizyczny', 'i', 'społeczny', 'jednocześnie', 'być', 'to', ',', 'co', 'stanowić', 'o', 'wymiar', 'życie', 'i', 'los', '.', 'Paźniewski', 'w', '„', 'krótki', 'dzień', '”', 'ofiarować', 'Kresy', 'nie', 'mało', ',', 'niż', 'z', 'on', 'zaczerpnąć', '.', 'zatrzymać', 'potop', '.', 'zamówić', 'kataklizm', '.', 'stworzyć', 'wizja', 'oczekiwanie', ',', 'wizja', 'spokój', 'przed', 'burza', '.', 'z', 'napięcie', 'czekać', 'na', 'chwila', ',', 'który', 'być', 'chwila', 'decydować', 'o', 'los', 'bohater', '.', 'cóż za', 'ulga', '.', 'mijać', 'ostatni', 'zdanie', 'powieść', '.', 'co za', 'wspaniały', 'książka', '!', 'ocalać', '!', '„plama”', 'Piętak', ',', 'jeden', 'spośród', 'kilka', 'znakomity', 'współczesny', 'powieść', ',', 'także', 'ze względu na', 'on', 'zaklasyfikować', 'wraz z', 'cały', 'twórczość', 'ten', 'pisarz', 'do', 'nurt', 'wiejski', ',', 'mieć', 'w', 'odbiór', 'powszechny', 'ten', 'ranga', ',', 'jaki', 'rzeczywiście', 'posiadać', '.', 'wszystko', 'co', 'Piętak', 'wynieść', 'z', 'chłopski', 'szkoła', 'wyobraźnia', ',', 'zaowocować', 'w', '„plama”', 'wysoki', 'subtelność', 'psychologiczny', ',', 'na', 'jaki', 'stać', 'literatura', '.', 'to', 'już', 'wątpliwość', 'religijny', ',,', 'ten', 'wątpliwość', 'pierwszy', 'stopień', 'wtajemniczenie', 'w', 'sprawa', 'świat', 'wątpliwość', '„', 'niebo', 'w', 'płomień', '”', 'czy', '„', 'Jan', 'Barois', '”', '.', 'tu', 'chodzić', 'o', 'sensowność', 'dogmat', 'czy', 'ścisłość', 'religijny', 'wyobrażenie', ',', 'chodzić', 'już', 'o', 'religia', ',', 'o', 'tajemnica', 'stworzenie', ',', 'ale', 'o', 'norma', 'etyczny', '.', 'kto', 'on', 'ustanowić', ',', 'kiedy', 'Bóg', 'zabraknąć', '?']\n"
     ]
    }
   ],
   "source": [
    "print(fragms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_line = True\n",
    "word_n = 0\n",
    "word_idx = {}\n",
    "\n",
    "# we'll read those from the data file\n",
    "vecs_count = 0\n",
    "vecs_dim = 100\n",
    "\n",
    "with open(vecs_path+\"data\") as vecs_file:\n",
    "    for line in vecs_file:\n",
    "        if first_line:\n",
    "            # Read metadata.\n",
    "            vecs_count = int(line.split(' ')[0])\n",
    "            vecs_dim = int(line.split(' ')[1])\n",
    "            first_line = False\n",
    "            continue\n",
    "        # Read lemma base forms.\n",
    "        word_idx[line.split(' ')[0]] = word_n\n",
    "        word_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vecs = np.loadtxt(vecs_path+\"data\", encoding=\"utf-8\",\n",
    "                       dtype=np.float32, # tensorflow's requirement\n",
    "                       skiprows=1, usecols=tuple(range(1, vecs_dim+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the dummy boundary/unknown marker.\n",
    "word_vecs = np.vstack([word_vecs, np.zeros((1,vecs_dim), dtype=np.float32)])\n",
    "vecs_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_id(word):\n",
    "    return word_idx[word] if word in word_idx else vecs_count-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1549322"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id('ffggf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1549323, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_count, vecs_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Training corpus preparation.\n",
    "#\n",
    "\n",
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vecs_count - 1 # the zero additional vector\n",
    "\n",
    "sample_n = 0\n",
    "\n",
    "train = np.zeros(((words_count + words_count // points_per_neg_sample) * corp_runs,\n",
    "                  window_size * 2 + 1), dtype='int')\n",
    "labels = np.ones(((words_count + words_count // points_per_neg_sample) * corp_runs,),\n",
    "                dtype='int')\n",
    "\n",
    "for run_n in range(corp_runs):\n",
    "    fragm_n = 0\n",
    "    word_n = 0\n",
    "        \n",
    "    while fragm_n < len(fragms) and sample_n < train.shape[0]:\n",
    "        \n",
    "        # The positive sample.\n",
    "        train[sample_n, window_size] = word_id(fragms[fragm_n][word_n])\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            train[sample_n, j] = (word_id(fragms[fragm_n][word_n-j-1]) if word_n-j-1 >= 0\n",
    "                                  else bound_token_id)\n",
    "            train[sample_n, window_size+j+1] = (word_id(fragms[fragm_n][word_n+j+1])\n",
    "                                                if word_n+j+1 < len(fragms[fragm_n])\n",
    "                                                else bound_token_id)\n",
    "        \n",
    "        # (Maybe) a negative sample.\n",
    "        if word_n % points_per_neg_sample == 0:\n",
    "            sample_n += 1\n",
    "            \n",
    "            neg_sample = []\n",
    "            while len(neg_sample) < 1 + (2 * window_size):\n",
    "                neg_sample.append(randint(0, vecs_count-1))\n",
    "                \n",
    "            train[sample_n,] = np.asarray(neg_sample, dtype='int')\n",
    "            labels[sample_n] = 0.0\n",
    "                \n",
    "        sample_n += 1\n",
    "        word_n += 1\n",
    "        try:\n",
    "            while word_n == len(fragms[fragm_n]):\n",
    "                word_n = 0\n",
    "                fragm_n += 1\n",
    "        except IndexError: # happens on the end of the corpus\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()                                                                                               \n",
    "model.add(Embedding(vecs_count,\n",
    "                    vecs_dim,\n",
    "                    weights=[word_vecs],\n",
    "                    input_length=window_size * 2 + 1,\n",
    "                    trainable=False))                                                                              \n",
    "model.add(LSTM(96))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "opt = SGD(lr=learning_rate, decay=reg_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3969276/3969276 [==============================] - 1058s 267us/step - loss: 0.0038 - acc: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f464956ed68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fc56c618860>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray([word_id(w) for w in\n",
    "                ['Niemcy', 'znienacka', 'wkroczyć', 'do', 'Francja', 'w', 'maj', 'kolejny', 'rok']],\n",
    "               dtype='int')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 3133,  272, 1251, 2252,    8,  102,   96,   57])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray([word_id(w) for w in\n",
    "                ['kot', 'zimny', 'okrągły', 'start', 'do', 'w', 'Czechy', 'wykres', 'klub']],\n",
    "               dtype='int')\n",
    "Y\n",
    "Y = np.asarray([word_id(w) for w in\n",
    "                ['w', 'zimny', 'miesiąc', 'zakładać', 'but', 'do', 'szkoła', 'każdy', 'dzień']],\n",
    "               dtype='int')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2646184, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999851]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.atleast_2d(Y), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from math import floor\n",
    "\n",
    "fragms_step = 1 # set to higher values if we want to skip some proportion of fragments\n",
    "# We need a special token for cases when the target word is near the start or end of sentence.\n",
    "bound_token_id = vecs_count - 1 # the zero additional vector\n",
    "\n",
    "def skipgram_batches():\n",
    "    for run_n in range(corp_runs):\n",
    "        sent_n = 0\n",
    "        word_n = 0\n",
    "        \n",
    "        target_n = 0 # relative to the current batch\n",
    "        \n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        \n",
    "        while sent_n < len(fragms):\n",
    "            for j in range(window_size):\n",
    "                batch[target_n*window_size+j] = word_id(fragms[sent_n][word_n])\n",
    "            # \"Good\" examples - words near the target (we will let TensorFlow randomize the \"bad\" ones)\n",
    "            for j in range(window_size // 2):\n",
    "                labels[target_n*window_size+j*2] = (word_id(fragms[sent_n][word_n-j-1]) if word_n-j-1 >= 0\n",
    "                                                       else bound_token_id)\n",
    "                labels[target_n*window_size+j*2+1] = (word_id(fragms[sent_n][word_n+j+1])\n",
    "                                                         if word_n+j+1 < len(fragms[sent_n])\n",
    "                                                         else bound_token_id)\n",
    "                \n",
    "            target_n += 1\n",
    "            if target_n == (batch_size // window_size):\n",
    "                yield batch, labels\n",
    "                batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "                target_n = 0\n",
    "                \n",
    "            word_n += 1\n",
    "            try:\n",
    "                while word_n == len(fragms[sent_n]):\n",
    "                    word_n = 0\n",
    "                    sent_n += fragms_step\n",
    "                    if (floor(sent_n / len(fragms) * 10)\n",
    "                        > floor((sent_n-fragms_step) / len(fragms) * 10)):\n",
    "                        print('{}0%'.format(floor(sent_n / len(fragms) * 10)), end=' ')\n",
    "            except IndexError: # happens on the end of the corpus\n",
    "                break\n",
    "                \n",
    "        batch[target_n:] = 0.0\n",
    "        labels[target_n:, :] = 0.0\n",
    "        yield batch, labels#, (run_n == corp_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classif_W1 = Variable(torch.randn(vecs_dim*2, vecs_dim*2), requires_grad=True)\n",
    "classif_b1 = Variable(torch.randn(1, vecs_dim*2), requires_grad=True)\n",
    "classif_W2 = Variable(torch.randn(vecs_dim*2, 1), requires_grad=True)\n",
    "classif_b2 = Variable(torch.randn(1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2018-02-21 22:48:45.389630\n",
      "Loss: Variable containing:\n",
      " 319.7979\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2371\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2371\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "10% Loss: Variable containing:\n",
      " 1.2368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "20% 30% 40% Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "50% Loss: Variable containing:\n",
      " 1.2370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "60% Loss: Variable containing:\n",
      " 1.2370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2367\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "70% Loss: Variable containing:\n",
      " 1.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "80% 90% Loss: Variable containing:\n",
      " 1.2368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 1.2370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "100% "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f7a3d9d7d8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training start:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskipgram_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# We get word indices, convert them to vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# main words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d3f938c02850>\u001b[0m in \u001b[0;36mskipgram_batches\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_n\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_n\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;31m#, (run_n == corp_runs - 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "batch_n = 0\n",
    "\n",
    "print('Training start:', datetime.datetime.now())\n",
    "for batch, labels in skipgram_batches():\n",
    "    # We get word indices, convert them to vectors.\n",
    "    batch = word_vecs[batch] # main words\n",
    "    pos_examples = word_vecs[labels]\n",
    "    neg_examples = word_vecs[np.random.randint(vecs_count, size=pos_examples.shape)]\n",
    "    \n",
    "    pos_batch = Variable(torch.Tensor(np.hstack((batch, pos_examples))), requires_grad=False)\n",
    "    pos_preds = ((pos_batch.mm(classif_W1) + classif_b1).sigmoid().mm(classif_W2)\n",
    "                      + classif_b2).sigmoid()\n",
    "    neg_batch = Variable(torch.Tensor(np.hstack((batch, pos_examples))), requires_grad=False)\n",
    "    neg_preds = (((neg_batch.mm(classif_W1) + classif_b1).sigmoid()).mm(classif_W2)\n",
    "                      + classif_b2).sigmoid()\n",
    "    \n",
    "    loss1 = ((- pos_preds).sum() / batch_size +\n",
    "             # regularization:\n",
    "                (classif_W1.abs().sum() + classif_b1.abs().sum() +\n",
    "                 classif_W2.abs().sum() + classif_b2.abs().sum())\n",
    "                * reg_rate)\n",
    "    loss1.backward()\n",
    "    loss2 = (neg_preds.sum() / (batch_size * 10) +\n",
    "             # regularization:\n",
    "                (classif_W1.abs().sum() + classif_b1.abs().sum() +\n",
    "                 classif_W2.abs().sum() + classif_b2.abs().sum())\n",
    "                * reg_rate)\n",
    "    loss2.backward()\n",
    "    if batch_n % 25000 == 0:\n",
    "        print(\"Loss: {}\".format(loss1 + loss2))\n",
    "        #print(pos_preds)\n",
    "        #print(neg_preds)\n",
    "    \n",
    "    classif_W1.data = learning_rate * classif_W1.grad.data\n",
    "    classif_b1.data = learning_rate * classif_b1.grad.data\n",
    "    classif_W2.data = learning_rate * classif_W2.grad.data\n",
    "    classif_b2.data = learning_rate * classif_b2.grad.data\n",
    "    classif_W1.grad.data.zero_()\n",
    "    classif_b1.grad.data.zero_()\n",
    "    classif_W2.grad.data.zero_()\n",
    "    classif_b2.grad.data.zero_()\n",
    "    \n",
    "    batch_n += 1\n",
    "print('Training end:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word = \"być\"\n",
    "softmax = torch.nn.Softmax()\n",
    "vec = Variable(torch.Tensor(\n",
    "            np.hstack((np.broadcast_to(word_vecs[word_id(word), :], (vecs_count, vecs_dim)),\n",
    "                       word_vecs))\n",
    "         ), requires_grad=False)\n",
    "pred = softmax((((vec.mm(classif_W1) + classif_b1).sigmoid()).mm(classif_W2)\n",
    "                      + classif_b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3104\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow (currently unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # Model parameters: word embeddings and model weights & biases for each word.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vecs_count, vecs_dim], -1.0, 1.0))\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vecs_count, vecs_dim],\n",
    "                                                  stddev=1.0 / math.sqrt(vecs_dim)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vecs_count]))\n",
    "    \n",
    "    # The computation graph.\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # Note that word2vec has no \"real\" hidden layers apart from the embedding.\n",
    "    \n",
    "    # Number of random words to sample apart from the true target; the model should learn to\n",
    "    # assign low probability to them given the context.\n",
    "    negative_samples_n = batch_size\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=labels,\n",
    "                                         inputs=embedding_layer,\n",
    "                                         num_sampled=negative_samples_n,\n",
    "                                         num_classes=vecs_count))\n",
    "    # Vanilla SGD seems to work here better - since we train practically a different word vector\n",
    "    # each time, decaying momentum hinders training of later vectors before they can even be shown\n",
    "    # to the net, especially in the case of Adagrad's vanishing updates.\n",
    "    # (NOTE!) here we DO NOT touch the embeddings, we want to only learn nce_weights and biases!\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2018-02-07 19:02:51.118178\n",
      "(loss: 675.9548950195312) 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% (loss: 2.3737263679504395) 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 10% 20% 30% 40% 50% (loss: 1.877175211906433) 60% 70% 80% 90% 100% Final loss: 2.2043426\n",
      "Training end: 2018-02-07 19:25:10.739346\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# we want to use those later:\n",
    "trained_nce_weights = []\n",
    "trained_nce_biases = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Training start:', datetime.datetime.now())\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    for batch_inputs, batch_labels, is_last in skipgram_batches():\n",
    "        if is_last:\n",
    "            _, loss_val, trained_nce_weights, trained_nce_biases = sess.run(\n",
    "                [optimizer, loss, nce_weights, nce_biases],\n",
    "                feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "            print('Final loss:', loss_val)\n",
    "            print('Training end:', datetime.datetime.now())\n",
    "        else:\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                             labels: batch_labels})\n",
    "            if (i % 250000 == 0):\n",
    "                print('(loss: {})'.format(loss_val), end=' ')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55766, 100), (55766,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_nce_weights.shape, trained_nce_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_words = [rev_word_idx[i] if i < vecs_count-1 else ' ' for i in range(idxs.shape[1])][::-1] # reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = \"być\"\n",
    "vec = np.reshape(word_vecs[word_id(word), :], (1, vecs_dim))\n",
    "prediction = np.dot(vec, np.transpose(trained_nce_weights))\n",
    "prediction = np.add(prediction, trained_nce_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31166"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs = np.argsort(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 55766)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'kazirodczy',\n",
       " 'teori',\n",
       " 'nscrossen',\n",
       " 'Bierówka',\n",
       " 'hat-tricka',\n",
       " 'bazgrać',\n",
       " 'rozgościć',\n",
       " 'niezmącony',\n",
       " 'Valle',\n",
       " 'łajać',\n",
       " '7-13',\n",
       " 'Brett',\n",
       " 'Silesii',\n",
       " 'zaliczkowy',\n",
       " 'Wojcieszów',\n",
       " 'Sław',\n",
       " 'odwiecznie',\n",
       " '²',\n",
       " 'kwalifikator',\n",
       " 'stulejka',\n",
       " 'Eugen',\n",
       " 'avi',\n",
       " 'abażur',\n",
       " 'wydeptywać',\n",
       " 'Dederko',\n",
       " 'płatowiec',\n",
       " 'niedosłuch',\n",
       " 'Kornasiewicz',\n",
       " 'Krysiewicza',\n",
       " 'Besbir',\n",
       " 'separować',\n",
       " 'Rumsfeld',\n",
       " 'Kurowo',\n",
       " 'obstrzał',\n",
       " 'Galapagos',\n",
       " 'odmownie',\n",
       " 'Dłutów',\n",
       " 'wolnomularski',\n",
       " 'Liptak',\n",
       " 'zbutwiały',\n",
       " 'Gliwa',\n",
       " 'Johana',\n",
       " 'tia',\n",
       " 'Cieślikowski',\n",
       " 'doktrynerstwo',\n",
       " 'nieokrzesany',\n",
       " '1440',\n",
       " 'EG',\n",
       " 'Lokia',\n",
       " 'Elo',\n",
       " 'budzetu',\n",
       " 'UFK',\n",
       " '70-300',\n",
       " 'wszystkowiedzący',\n",
       " 'Burger',\n",
       " 'ankiete',\n",
       " 'Małastowskiej',\n",
       " 'Kubiś',\n",
       " 'rozpustny',\n",
       " 'Galos',\n",
       " '75-300',\n",
       " 'JKK',\n",
       " '637-12-23',\n",
       " 'płaszczak',\n",
       " 'wine',\n",
       " 'ats',\n",
       " 'żupan',\n",
       " 'zauwazyl',\n",
       " 'wykusz',\n",
       " \"There's\",\n",
       " 'Solar',\n",
       " 'skamleć',\n",
       " 'szczotkować',\n",
       " 'Karamazow',\n",
       " 'gromnica',\n",
       " 'dysgrafia',\n",
       " 'łyżworolka',\n",
       " 'Zarzycka',\n",
       " 'Muskat',\n",
       " 'choleryk',\n",
       " 'Matteo',\n",
       " 'Basiak',\n",
       " 'browning',\n",
       " 'wywiadownia',\n",
       " 'Ravensbrück',\n",
       " 'Lubomirska',\n",
       " 'KDE',\n",
       " 'Claus',\n",
       " 'Hucina',\n",
       " 'Wizygot',\n",
       " 'Hg',\n",
       " 'Knysok',\n",
       " 'zaparzać',\n",
       " 'odbezpieczyć',\n",
       " 'Sanders',\n",
       " 'Kriegsmarine',\n",
       " 'Record',\n",
       " 'zapamiętale',\n",
       " 'Paper']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
